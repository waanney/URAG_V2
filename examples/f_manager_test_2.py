# -*- coding: utf-8 -*-
"""
Quick & Simple Runner for FManager (no CLI fuss)
================================================
Cháº¡y tháº³ng: `python -m examples.run_f_manager_quicktest`

Ká»‹ch báº£n:
- Thá»­ láº¥y augmented tá»« `./augmented_quicktest.jsonl` (náº¿u cÃ³).
- Náº¿u khÃ´ng cÃ³, Ä‘á»c `./data/docs` Ä‘á»ƒ tá»± táº¡o augmented tá»‘i thiá»ƒu (original=transformed=text).
- DÃ¹ng `SimpleFaqGenerator` (heuristic) Ä‘á»ƒ táº¡o FAQ gá»‘c + paraphrase.
- DÃ¹ng `EmbedderAdapter` (bá»c `embedding.EmbedderAgent`) Ä‘á»ƒ encode.
- Upsert vÃ o Milvus báº±ng `IndexingAgent` (Ä‘áº¿n collection `*_quickfaq__faq`).
- In summary + stats, rá»“i dá»n collection.
"""
from __future__ import annotations
import os, json, time
from pathlib import Path
from typing import Any, Dict, List

from src.managers.urag_f_manager import FManager, FManagerConfig
from src.managers.urag_f_manager import AugmentedChunk, FAQItem, FAQWithVec, IFaqGenerator, IEmbedder
from src.indexing.indexing_agent import IndexingAgent, AgentConfig, StatsReq
from pymilvus import utility

# ---- Embedder adapter (ðŸ†• bá»c EmbedderAgent Ä‘á»ƒ khá»›p IEmbedder) ----
from embedding.embedding_agent import EmbConfig, EmbedderAgent


class EmbedderAdapter(IEmbedder):  # type: ignore[misc]
    def __init__(self, language: str = "default") -> None:
        self.agent = EmbedderAgent(
            EmbConfig(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                vi_model_name="dangvantuan/vietnamese-embedding",
                language=("vi" if language == "vi" else "default"),
                normalize_for_cosine=True,
                metric="COSINE",
            )
        )

    def encode_texts(self, texts: List[str]) -> List[List[float]]:
        return self.agent.encode(texts)  # type: ignore[attr-defined]

    def info(self) -> Dict[str, Any]:
        return {"embedder": "EmbedderAgent", "dim_hint": 384}


# ---- Simple FAQ generator (ðŸ†• heuristic, khÃ´ng cáº§n LLM) ----
class SimpleFaqGenerator(IFaqGenerator):  # type: ignore[misc]
    def generate_roots(self, augmented_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        roots: List[Dict[str, Any]] = []
        for row in augmented_chunks:
            doc_id = str(row.get("doc_id", "doc"))
            chunk_id = str(row.get("chunk_id", "0"))
            # Láº¥y text tá»« cÃ¡c khÃ³a phá»• biáº¿n (transformed/original/text)
            text = str(row.get("text") or row.get("transformed") or row.get("original") or "").strip()
            if not text:
                continue
            q = f"Ná»™i dung chÃ­nh cá»§a Ä‘oáº¡n {chunk_id} lÃ  gÃ¬?"
            a = text if len(text) <= 512 else (text[:512] + "â€¦")
            roots.append({
                "question": q,
                "answer": a,
                "canonical_id": f"{doc_id}::{chunk_id}",
                "source": "faq_src",
                "metadata": {"from_chunk": chunk_id, "doc_id": doc_id}
            })
        return roots

    def enrich_from_roots(self, roots: List[Dict[str, Any]], paraphrase_n: int = 5) -> List[Dict[str, Any]]:
        out: List[Dict[str, Any]] = []
        for r in roots:
            out.append({**r, "is_paraphrase": False})  # giá»¯ root
            base_q = r["question"]
            a = r["answer"]
            # táº¡o vÃ i paraphrase Ä‘Æ¡n giáº£n
            templates = [
                "TÃ³m táº¯t ná»™i dung Ä‘oáº¡n nÃ y lÃ  gÃ¬?",
                "Äoáº¡n vÄƒn nÃ y nÃ³i vá» Ä‘iá»u gÃ¬?",
                "Äiá»ƒm chÃ­nh cá»§a Ä‘oáº¡n lÃ  gÃ¬?",
                "Báº¡n cÃ³ thá»ƒ giáº£i thÃ­ch Ä‘oáº¡n nÃ y khÃ´ng?",
                "Ná»™i dung Ä‘oáº¡n vÄƒn táº­p trung vÃ o váº¥n Ä‘á» nÃ o?",
            ]
            for i, t in enumerate(templates[:paraphrase_n]):
                out.append({
                    **r,
                    "question": t,
                    "answer": a,
                    "is_paraphrase": True,
                })
        return out


# ---- Utilities ----
AUG_PATH = Path("./augmented_quicktest.jsonl")
DOCS_DIR = Path("./data/docs")
LANGUAGE = os.getenv("URAG_LANG", "default")
COLLECTION_BASE = f"ura_rag_quickfaq_{int(time.time())}"
DO_CLEANUP = True


def load_augmented() -> List[Dict[str, Any]]:
    if AUG_PATH.exists():
        data: List[Dict[str, Any]] = []
        with AUG_PATH.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    data.append(json.loads(line))
                except Exception:
                    pass
        if data:
            print(f"[QuickFAQ] Loaded augmented from {AUG_PATH} -> {len(data)} items")
            return data
    # Fallback: táº¡o augmented tá»‘i thiá»ƒu tá»« docs
    print("[QuickFAQ] No augmented file found, fallback to docs â†’ minimal augmented")
    DOCS_DIR.mkdir(parents=True, exist_ok=True)
    if not any(DOCS_DIR.iterdir()):
        (DOCS_DIR / "hello.txt").write_text("Xin chÃ o, Ä‘Ã¢y lÃ  tÃ i liá»‡u máº«u cho FManager.", encoding="utf-8")
    out: List[Dict[str, Any]] = []
    for i, p in enumerate(sorted(DOCS_DIR.glob("**/*"))):
        if p.is_dir():
            continue
        try:
            text = p.read_text(encoding="utf-8")
        except Exception:
            continue
        out.append({
            "doc_id": p.stem,
            "chunk_id": f"c{i}",
            "original": text,
            "transformed": text,
            "metadata": {"path": str(p)}
        })
    print(f"[QuickFAQ] Built minimal augmented: {len(out)} items")
    return out


def main() -> int:
    print("[QuickFAQ] Startingâ€¦")

    # 1) Chuáº©n bá»‹ components
    faq_gen = SimpleFaqGenerator()
    embedder = EmbedderAdapter(language=LANGUAGE)
    fcfg = FManagerConfig(embed_field="question", l2_normalize=True)
    fm = FManager(faq_gen, embedder, fcfg)

    # 2) Load augmented
    augmented = load_augmented()

    # 3) Run full pipeline (augmented â†’ roots â†’ enrich â†’ embed â†’ index)
    result = fm.run_from_augmented(
        augmented,
        collection_base=COLLECTION_BASE,
        paraphrase_n=3,
        metric="COSINE",
        index_params=None,
        shards=2,
        build_index=True,
    )

    print("\n=== Summary ===")
    print(json.dumps(result.get("summary", {}), ensure_ascii=False, indent=2))

    # 4) Stats nhanh (FAQ collection)
    try:
        ia = IndexingAgent(AgentConfig())
        faq_col = COLLECTION_BASE + "__faq"
        stats = ia.process(StatsReq(op="stats", collection=faq_col).model_dump())
        print("\n=== FAQ Stats ===")
        print(json.dumps(stats, ensure_ascii=False, indent=2))
    except Exception as e:
        print(f"[WARN] stats failed: {e}")

    # 5) Cleanup
    if DO_CLEANUP:
        try:
            for name in [COLLECTION_BASE + "__doc", COLLECTION_BASE + "__faq", COLLECTION_BASE]:
                if utility.has_collection(name):
                    utility.drop_collection(name)
                    print(f"[Cleanup] Dropped collection: {name}")
        except Exception as e:
            print(f"[WARN] cleanup failed: {e}")

    print("\n[QuickFAQ] Done.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
