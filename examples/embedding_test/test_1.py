# examples/embedding_test/embedding_test_1.py
from __future__ import annotations
import os, json, time, random, string
from pathlib import Path

# ðŸ‘‰ chá»‰nh láº¡i import nÃ y theo nÆ¡i báº¡n Ä‘áº·t file embedder_agent.py
# vÃ­ dá»¥: from src.embedding.embedder_agent import EmbedderAgent, EmbConfig
from src.llm.embedding_agent import EmbedderAgent, EmbConfig

def rand_suffix(n=6):
    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=n))

# === cáº¥u hÃ¬nh embedder (COSINE + normalize Ä‘á»ƒ há»£p vá»›i Milvus COSINE) ===
cfg = EmbConfig(
    model_name=os.getenv("EMB_MODEL", "sentence-transformers/all-MiniLM-L6-v2"),
    metric="COSINE",
    normalize_for_cosine=True,
    batch_size=64,
)
agent = EmbedderAgent(cfg)

print("== EMBEDDER AGENT INFO ==")
print(f"model: {cfg.model_name}")
print(f"dim:   {agent.dim}")
print()

# --------- dá»¯ liá»‡u máº«u ----------
docs_input = [
    # cÃ³ thá»ƒ truyá»n list[str]
    "TrÆ°á»ng ÄH BÃ¡ch Khoa cÃ³ 2 cÆ¡ sá»Ÿ. CÆ¡ sá»Ÿ 1 á»Ÿ quáº­n 10, cÆ¡ sá»Ÿ 2 á»Ÿ Thá»§ Äá»©c.",
    # hoáº·c list[dict] cÃ³ 'text' + metadata tuá»³ Ã½ (KHÃ”NG cÃ³ id)
    {
        "text": "Thá»i háº¡n Ä‘Ã³ng há»c phÃ­ lÃ  trÆ°á»›c ngÃ y 15 hÃ ng thÃ¡ng.",
        "source": "handbook.md",
        "metadata": {"sensitivity": "critical"}
    },
]

faqs_input = [
    {"question": "BK cÃ³ bao nhiÃªu cÆ¡ sá»Ÿ?", "answer": "HCMUT cÃ³ 2 cÆ¡ sá»Ÿ (Q10 vÃ  Thá»§ Äá»©c)."},
    {"question": "Háº¡n chÃ³t Ä‘Ã³ng há»c phÃ­ khi nÃ o?", "answer": "TrÆ°á»›c ngÃ y 15 hÃ ng thÃ¡ng."},
]

# --------- embed ---------
doc_with_vec = agent.embed_docs(docs_input, default_source="doc_src")
faq_with_vec = agent.embed_faqs(faqs_input, default_source="faq_src")

# --------- in tÃ³m táº¯t káº¿t quáº£ (áº©n bá»›t vector cho gá»n) ---------
def preview(item, k=6):
    v = item.get("vector", [])[:k]
    show = {**{k: v for k, v in item.items() if k != "vector"}}
    show["vector_preview"] = v
    show["vector_dim"] = len(item.get("vector", []))
    return show

print("== DOC EMBEDS ==")
for i, it in enumerate(doc_with_vec, 1):
    print(json.dumps(preview(it), ensure_ascii=False, indent=2))

print("\n== FAQ EMBEDS ==")
for i, it in enumerate(faq_with_vec, 1):
    print(json.dumps(preview(it), ensure_ascii=False, indent=2))

# --------- lÆ°u JSON Ä‘á»ƒ feed cho IndexingAgent (khÃ´ng cÃ³ id) ---------
out_dir = Path("examples/embedding_test/tmp_embeds")
out_dir.mkdir(parents=True, exist_ok=True)
suffix = f"{int(time.time())}_{rand_suffix()}"
doc_out = out_dir / f"docs_with_vec_{suffix}.json"
faq_out = out_dir / f"faqs_with_vec_{suffix}.json"

with open(doc_out, "w", encoding="utf-8") as f:
    json.dump(doc_with_vec, f, ensure_ascii=False, indent=2)
with open(faq_out, "w", encoding="utf-8") as f:
    json.dump(faq_with_vec, f, ensure_ascii=False, indent=2)

print(f"\n== WROTE FILES ==")
print(f"- {doc_out}")
print(f"- {faq_out}")

# Gá»£i Ã½: á»Ÿ bÆ°á»›c tiáº¿p theo, báº¡n cÃ³ thá»ƒ:
# 1) map má»—i item -> thÃªm 'id' (vÃ­ dá»¥ uuid4), 
# 2) gá»­i tá»«ng nhÃ³m sang IndexingAgent:
#    - DOC:  collection = <base_name>__doc, items: [{id, type='doc', text, vector, source, metadata, ts}]
#    - FAQ:  collection = <base_name>__faq, items: [{id, type='faq', question, answer, vector, source, metadata, ts}]
# 3) giá»¯ nguyÃªn metric COSINE Ä‘á»ƒ khá»›p normalize á»Ÿ Ä‘Ã¢y.
