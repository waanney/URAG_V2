# -*- coding: utf-8 -*-
# Streamlit Milvus Inspector ‚Äî READ-ONLY dashboard
#
# T√≠nh nƒÉng:
# 1) K·∫øt n·ªëi Milvus (URI/TOKEN)
# 2) Li·ªát k√™ collections
# 3) Xem chi ti·∫øt 1 collection: schema, index, num_entities
# 4) Duy·ªát m·∫´u (query limit N) an to√†n theo schema (DOC/FAQ)
# 5) Search nhanh b·∫±ng EmbedderAgent (nh·∫≠p text -> embed -> search)
#
# Y√™u c·∫ßu:
#   pip/uv install streamlit pymilvus sentence-transformers torch (n·∫øu d√πng GPU)
#   v√† project c·ªßa b·∫°n trong PYTHONPATH (ƒë·ªÉ import EmbedderAgent/IndexingAgent)
#
# Ch·∫°y:
#   uv run streamlit run src/UI/milvus_stalk.py
#
import sys
from pathlib import Path

# .../URAG_V2/src/UI/milvus_stalk.py -> project root = .../URAG_V2
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

import os
import json
import math
import streamlit as st
from typing import Any, Dict, List, Optional, cast

from pymilvus import connections, utility, Collection
# M·ªôt s·ªë phi√™n b·∫£n pymilvus d√πng SearchFuture cho search async
try:
    # path t√πy version; kh·ªëi try/except gi√∫p Pylance hi·ªÉu ƒë∆∞·ª£c type
    from pymilvus.orm.future import SearchFuture  # type: ignore
except Exception:
    class SearchFuture:  # fallback ƒë·ªÉ type-checking kh√¥ng b√°o l·ªói
        pass

# ---- import t·ª´ codebase c·ªßa b·∫°n ----
from src.embedding.embedding_agent import EmbedderAgent, EmbConfig, Language
from src.indexing.indexing_agent import AgentConfig as IdxCfg  # ch·ªâ ƒë·ªÉ l·∫•y default k·∫øt n·ªëi




# ----------------------------- helpers -----------------------------
def safe_connect(uri: str, token: Optional[str], alias: str = "default") -> bool:
    try:
        kwargs: Dict[str, Any] = {"alias": alias, "uri": uri}
        if token:
            kwargs["token"] = token
        connections.connect(**kwargs)
        return True
    except Exception as e:
        st.error(f"Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c Milvus: {e}")
        return False


def get_schema_dict(col: Collection) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    for f in col.schema.fields:
        pd = getattr(f, "params", {}) or {}
        rows.append({
            "name": f.name,
            "dtype": f.dtype,
            "is_primary": getattr(f, "is_primary", False),
            "auto_id": getattr(f, "auto_id", False),
            "max_length": getattr(f, "max_length", None),
            "params": pd
        })
    return rows


def get_index_info(col: Collection) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    try:
        for idx in (col.indexes or []):
            out.append({
                "field": idx.field_name,
                "index_type": idx.params.get("index_type"),
                "metric_type": idx.params.get("metric_type"),
                "params": {k: v for k, v in idx.params.items() if k not in ("index_type", "metric_type")}
            })
    except Exception as e:
        st.warning(f"Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c index info: {e}")
    return out


def get_vector_dim(col: Collection) -> int:
    for f in col.schema.fields:
        if f.name == "vector":
            return int(f.params["dim"])
    raise RuntimeError("Kh√¥ng t√¨m th·∫•y tr∆∞·ªùng vector")


def l2_normalize(v: List[float]) -> List[float]:
    s = sum(x * x for x in v)
    if s <= 0:
        return v
    inv = 1.0 / math.sqrt(s)
    return [x * inv for x in v]


def guess_is_faq(col: Collection) -> bool:
    names = {f.name for f in col.schema.fields}
    return "question" in names and "answer" in names


def sample_rows(col: Collection, limit: int = 20) -> List[Dict[str, Any]]:
    """
    L·∫•y v√†i b·∫£n ghi ƒë·ªÉ xem. Ta c·ªë g·∫Øng d√πng expr an to√†n:
    - N·∫øu c√≥ 'ts' -> expr = "ts >= 0"
    - N·∫øu kh√¥ng, n·∫øu c√≥ 'id' -> expr = "id like '%'"
    """
    names = {f.name for f in col.schema.fields}
    if "ts" in names:
        expr = "ts >= 0"
    elif "id" in names:
        expr = "id like '%'"
    else:
        raise RuntimeError("Kh√¥ng c√≥ field ph√π h·ª£p ƒë·ªÉ query")

    # ch·ªçn output_fields ph√π h·ª£p
    if guess_is_faq(col):
        out_fields = [f for f in ["id", "question", "answer", "source", "metadata", "ts"] if f in names]
    else:
        out_fields = [f for f in ["id", "text", "source", "metadata", "ts"] if f in names]

    try:
        rows = col.query(expr=expr, output_fields=out_fields, limit=limit)
        # parse metadata JSON n·∫øu c·∫ßn
        for r in rows:
            if isinstance(r.get("metadata"), str):
                try:
                    r["metadata"] = json.loads(r["metadata"])
                except Exception:
                    pass
        return rows
    except Exception as e:
        st.warning(f"Query m·∫´u l·ªói: {e}")
        return []


def do_search(col: Collection, query_vec: List[float], top_k: int = 5, metric: str = "COSINE",
              output_fields: Optional[List[str]] = None) -> Dict[str, Any]:
    dim = get_vector_dim(col)
    if len(query_vec) != dim:
        return {"error": f"Dim kh√¥ng kh·ªõp: collection={dim}, query={len(query_vec)}"}

    # ƒëo√°n search params theo index
    sp: Dict[str, Any] = {}
    idx_type: Optional[str] = None
    try:
        if col.indexes:
            idx_type = col.indexes[0].params.get("index_type")
        if idx_type == "HNSW":
            sp = {"ef": 64}
        elif idx_type and idx_type.startswith("IVF"):
            sp = {"nprobe": 16}
    except Exception:
        pass

    # normalize n·∫øu cosine
    qv = query_vec[:]
    if metric.upper() == "COSINE":
        qv = l2_normalize(qv)

    names = {f.name for f in col.schema.fields}
    if not output_fields:
        if guess_is_faq(col):
            output_fields = [x for x in ["id", "question", "answer", "source", "metadata"] if x in names]
        else:
            output_fields = [x for x in ["id", "text", "source", "metadata"] if x in names]

    try:
        results: Any = col.search(
            data=[qv], anns_field="vector", param=sp, limit=top_k,
            output_fields=output_fields, consistency_level="Strong"
        )

        # N·∫øu ƒë·ªëi t∆∞·ª£ng c√≥ .result() (async/future), g·ªçi ƒë·ªÉ l·∫•y k·∫øt qu·∫£ th·ª±c
        if hasattr(results, "result"):
            try:
                results = results.result()
            except Exception:
                # N·∫øu .result() kh√¥ng kh·∫£ d·ª•ng th·ª±c s·ª±, c·ª© ƒë·ªÉ nguy√™n (m·ªôt s·ªë b·∫£n tr·∫£ list lu√¥n)
                pass

        hits_list = results[0]
        out: List[Dict[str, Any]] = []
        for h in hits_list:
            row: Dict[str, Any] = {"id": h.entity.get("id"), "distance": float(h.distance)}
            for f in (output_fields or []):
                try:
                    row[f] = h.entity.get(f)
                except Exception:
                    pass
            if isinstance(row.get("metadata"), str):
                try:
                    row["metadata"] = json.loads(row["metadata"])
                except Exception:
                    pass
            out.append(row)
        return {"index_type": idx_type, "search_params": sp, "results": out}
    except Exception as e:
        return {"error": str(e)}


# ----------------------------- UI -----------------------------
st.set_page_config(page_title="Milvus Inspector (URAG-V2)", layout="wide")

st.title("üîé Milvus Inspector ‚Äî URAG-V2 (read-only)")
st.caption("Xem nhanh collections, schema, index, ƒë·∫øm entities, duy·ªát m·∫´u & search th·ª≠.")

with st.sidebar:
    st.header("K·∫øt n·ªëi")
    default_uri = IdxCfg().uri  # l·∫•y default t·ª´ AgentConfig c·ªßa b·∫°n
    uri = st.text_input(
        "MILVUS URI",
        value=default_uri,
        help="VD: http://127.0.0.1:19530 ho·∫∑c milvus+https://... n·∫øu d√πng Milvus Cloud"
    )
    token_env = os.getenv("MILVUS_TOKEN")
    token = st.text_input("Token (n·∫øu c√≥)", type="password", value=(token_env or ""))

    if st.button("üîå Connect"):
        ok = safe_connect(uri, (token or None))
        st.session_state["connected"] = ok

    connected = st.session_state.get("connected", False)
    st.markdown("---")
    st.header("Embedder (Search)")
    model_name = st.text_input("Embedding model", value="BAAI/bge-m3")
    language_str = st.selectbox("Language", ["default", "vi", "auto"], index=0)
    top_k = st.number_input("Top-K", min_value=1, max_value=50, value=5, step=1)

if not st.session_state.get("connected"):
    st.info("H√£y k·∫øt n·ªëi Milvus ·ªü thanh b√™n tr√°i ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    st.stop()

# Health
ok = connections.has_connection("default")
st.success("ƒê√£ k·∫øt n·ªëi Milvus.") if ok else st.error("K·∫øt n·ªëi kh√¥ng ·ªïn.")

# List collections
cols = utility.list_collections()
st.subheader(f"üìö Collections ({len(cols)})")
st.write(cols if cols else "Kh√¥ng c√≥ collection n√†o.")

if not cols:
    st.stop()

# lu√¥n ch·ªçn index=0 ƒë·ªÉ tr√°nh None
sel = st.selectbox("Ch·ªçn collection ƒë·ªÉ xem chi ti·∫øt", options=cols, index=0)
if not sel:
    st.stop()

col = Collection(cast(str, sel))
col.load()

# Summary row
c1, c2, c3 = st.columns(3)
with c1:
    st.metric("Entities", value=f"{col.num_entities:,}")
with c2:
    try:
        st.metric("Dim", value=get_vector_dim(col))
    except Exception:
        st.metric("Dim", value="‚Äî")
with c3:
    st.metric("Type", value=("FAQ" if guess_is_faq(col) else "DOC"))

# Schema & Index
st.markdown("### üß± Schema")
st.dataframe(get_schema_dict(col), use_container_width=True)

st.markdown("### üßÆ Index")
idx_info = get_index_info(col)
if idx_info:
    st.json(idx_info)
else:
    st.write("Ch∆∞a c√≥ index (Milvus s·∫Ω search theo flat).")

# Browse sample
st.markdown("### üëÄ Duy·ªát m·∫´u")
limit = st.slider("S·ªë b·∫£n ghi", 1, 100, 20)
rows = sample_rows(col, limit=limit)
st.write(rows if rows else "Kh√¥ng l·∫•y ƒë∆∞·ª£c b·∫£n ghi n√†o (th·ª≠ tƒÉng limit ho·∫∑c ki·ªÉm tra expr).")

# Search quick test
st.markdown("### üîç Search nhanh (text ‚Üí embed ‚Üí search)")
query = st.text_area("Nh·∫≠p c√¢u ƒë·ªÉ t√¨m", placeholder="V√≠ d·ª•: ƒêi·ªÉm s√†n tuy·ªÉn sinh l√† bao nhi√™u?")

if st.button("Search"):
    try:
        # √âp ki·ªÉu Literal cho Pylance (Language = Literal["auto","vi","default"])
        lang_val: Language = cast(Language, language_str)

        embedder = EmbedderAgent(EmbConfig(model_name=model_name, language=lang_val))
        dim_embed = embedder.info()["dim"]
        vec = embedder.encode([query])[0]

        # N·∫øu dim l·ªách v·ªõi collection, c·∫£nh b√°o r√µ r√†ng
        dim_col = get_vector_dim(col)
        if dim_embed != dim_col:
            st.error(
                f"Dim mismatch: embedder={dim_embed}, collection={dim_col}. "
                f"ƒê·ªïi model ho·∫∑c ch·ªçn collection kh√°c cho kh·ªõp."
            )
        else:
            out = do_search(col, vec, top_k=top_k, metric="COSINE")
            if "error" in out:
                st.error(out["error"])
            else:
                st.write("Search params:", {"index_type": out["index_type"], **out["search_params"]})
                st.json(out["results"])
    except Exception as e:
        st.error(f"L·ªói search: {e}")
